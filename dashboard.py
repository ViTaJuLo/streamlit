# -*- coding: utf-8 -*-
"""Dashboard.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DisnWposypc8Uh2TXgkguCmnL_x01fY2

# Dashboard

# Setting up the dashboard
"""

# install streamlit
!pip install streamlit

pip install --upgrade streamlit

# install a specific pyngrok version (=4.1.10.)
# newer versions are not able to stream streamlit via google collab
# to stick to this version therefore is essential!
#!pip install pyngrok==4.1.10.

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import numpy as np
# import plotly.express as px
# import pandas as pd
# 
# ### MAIN PAGE #######
# # Title & subtitle for main page
# 
# #col1, mid, col2 = st.columns([1,1,20])
# #with col1:
#     #st.image('/content/edeka.jpeg', width=100)
# #with col2:
# st.image("/content/edeka.jpeg", use_column_width=False, width= 100)
# st.title("Customer Satisfaction Dashboard")
# st.subheader("Wie schneidet Edeka im Vergleich zu anderen Lebensmittelketten ab?" )
# st.caption ("basierend auf Google Reviews")
# # setting background picture 
# import base64 
# 
# def get_base64(bin_file):
#     with open(bin_file, 'rb') as f:
#         data = f.read()
#     return base64.b64encode(data).decode()
# 
# 
# def set_background(png_file):
#     bin_str = get_base64(png_file)
#     page_bg_img = '''
#     <style>
#     .stApp {
#     background-image: url("data:image/png;base64,%s");
#     background-size: cover;
#     }
#     </style>
#     ''' % bin_str
#     st.markdown(page_bg_img, unsafe_allow_html=True)
# 
# set_background('/content/background.jpg')
# 
# # Load data
# @st.cache
# # function to read in store data and combine them into one single dataframe
# def read_df1():
#   df = pd.read_excel('/content/merged_data.xlsx')
#   return df
# 
# def read_df2():
#   df = pd.read_excel("/content/nlp_df.xlsx")
#   return df 
# 
# # create the two dataframes for visuals
# merged_data = read_df1()
# nlp_df = read_df2()
# 
# # show dataframes  on streamlit
# #st.write(merged_data)
# st.write(nlp_df)
# 
# 
# 
# #### Side bar ####
# 
# # add sidebar with stores to compare
# st.sidebar.subheader("Visualisierungs-Möglichkeiten")
# 
# tickers = merged_data['new_place_id'].unique()
# dropdown = st.sidebar.multiselect('Welche Stores möchten Sie vergleichen?', tickers)
# 
# filtered_df = merged_data[merged_data["new_place_id"].isin(dropdown)]
# #filtered_df['owner_answer'] = filtered_df['owner_answer'].fillna(0)
# filtered_df2 = nlp_df[nlp_df["new_place_id"].isin(dropdown)]
# 
# #### Create Key Metrics #####
# st.markdown('### Key Metriken')
# 
# # computing average values for our metric boxes
# #avg_value_rating = int(sum(filtered_df['review_rating']) / len(filtered_df['review_rating']))
# #avg_value_sentiment = int(sum(filtered_df2['polarity']) / len(filtered_df2['polarity']) *100)
# #percentage_owner_a = int(sum(filtered_df['owner_answer']) / len(filtered_df['owner_answer'])*100)
# # creating the visual columns
# average_rating, average_sentiment_score, owner_answer_percentage = st.columns(3)
# 
# # filling columns with metrics
# average_rating.metric(label = "Durschnittliches Rating in Sternen", value=0, delta=0)
# 
# average_sentiment_score.metric(label = "Durschnittliches Sentiment %", value=0, delta = 0)
# 
# owner_answer_percentage.metric(label = "Antwortrate auf Reviews in %", value=0, delta = 0)
# 
# #### SHOW DATA SET ####
# 
# #st.markdown("### Zugrunde liegender Datensatz")
# #st.write(filtered_df)
# #### PLOTS ####
# 
# #### average rating 
# st.markdown("### Was ist die durschnittliche Bewertung der Geschäfte in Sternen pro Jahr?")
# fig = px.histogram(filtered_df, x="Review_year", y="review_rating",
#              color='new_place_id', barmode='group', histfunc='avg',
#              height=400, labels=dict(year="Jahr", review_rating="Bewertung in Sternen"))
# 
# st.plotly_chart(fig, use_container_width=True)
# 
# 
# 
# ### 
#

cols = ["name", "host_name", "neighbourhood", "room_type", "price"]
st_ms = st.multiselect("Columns", df.columns.tolist(), default=cols)

!ls

# authentificate Ngrok application which a unique token (connected to my free account)
!ngrok authtoken 29OSnFORcs4sZsixAMPxw0HuvfS_78LJukCmvyxk9Gk3V6KZa

# importing the application
from pyngrok import ngrok

# run the dashboard 'app'
!streamlit run app.py &>/dev/null&

# check if Streamlit is running
!pgrep streamlit

# Setup a tunnel to the streamlit port 8501
# the ouput URL can be copy pasted into Google Chrome 
# unfortunately it is somehow not working on Safari, so please stick to google chrome
public_url = ngrok.connect(port='8501')
public_url

# installing plotly for data visualization
!pip install plotly

# Commented out IPython magic to ensure Python compatibility.
# installing libraries for feature engineering and data visualization
import pandas as pd
import numpy as np
#import plotly.express as px

import matplotlib.pyplot as plt
# %matplotlib inline

"""# Data Import"""

# setting up pipeline to google drive where the data currently lies
from google.colab import drive

drive.mount('/content/drive')

# loading different data set per store 
allman = pd.read_excel('/content/drive/MyDrive/data/allmannsweiler.xlsx')
alten = pd.read_excel('/content/drive/MyDrive/data/altenheim.xlsx')
emmen2 = pd.read_excel('/content/drive/MyDrive/data/emmendingen2.xlsx')

"""import pandas as pd"""

import pandas as pd

"""# Data Cleaning & Basic Feature Engineering"""

# concatenating all dataframes into one 
merged_data = pd.concat([allman, emmen2, alten], ignore_index=True, axis=0)

merged_data.info()

def clean_data(dataframe):
   """ This function does the following:
   1. Deletes unnecessary columns
   2. Change place IDs into place names
   3. Convert time column to year, month, day
   """
   # deleting unnecessary columns, which are not givin any additional info for analysis 
   merged_data = merged_data.drop(columns=['query', 'google_id', 'reviews_link', 'location_link', 'review_id', 'autor_name', 'autor_image', 'autor_link' ,'review_img_url', 'owner_answer_timestamp_datetime_utc', 'review_link', 'review_timestamp'])
   merged_data['place_id'] = np.where((merged_data.place_id == 'ChIJtZTQCPYxkUcRXXPjA41wa-8'),'Allmannsweiler',merged_data.place_id)
   merged_data['place_id'] = np.where((merged_data.place_id == 'ChIJ5QMHFs8ZkUcRNjLn6iYPwHY'),'Emmendingen2',merged_data.place_id)
   merged_data['place_id'] = np.where((merged_data.place_id == 'ChIJU0cKGNHMlkcRntkOoyOZ09Y'),'Altenheim',merged_data.place_id)
   merged_data['Dates'] = pd.to_datetime(merged_data['review_datetime_utc']).dt.date
   merged_data['Time'] = pd.to_datetime(merged_data['review_datetime_utc']).dt.time
   merged_data['year'] = pd.DatetimeIndex(merged_data['Dates']).year
   merged_data['month'] = pd.DatetimeIndex(merged_data['Dates']).month
   merged_data['day'] = pd.DatetimeIndex(merged_data['Dates']).day

   return merged_data

# deleting unnecessary columns, which are not givin any additional info for analysis 
merged_data = merged_data.drop(columns=['query', 'google_id', 'reviews_link', 'location_link', 'review_id', 'autor_name', 'autor_image', 'autor_link' ,'review_img_url', 'owner_answer_timestamp_datetime_utc', 'review_link', 'review_timestamp'])

# Since we only have the google_id, which is a bit cryptic, we want to rename the google_id in the store name, which workers can decipher
# this needs to be abstracted / automized so we can put 65 files in here 
merged_data['place_id'] = np.where((merged_data.place_id == 'ChIJtZTQCPYxkUcRXXPjA41wa-8'),'Allmannsweiler',merged_data.place_id)
merged_data['place_id'] = np.where((merged_data.place_id == 'ChIJ5QMHFs8ZkUcRNjLn6iYPwHY'),'Emmendingen2',merged_data.place_id)
merged_data['place_id'] = np.where((merged_data.place_id == 'ChIJU0cKGNHMlkcRntkOoyOZ09Y'),'Altenheim',merged_data.place_id)

merged_data.head()

# splitting datetime stamp to make visuals more insightful / easier to work with
merged_data['Dates'] = pd.to_datetime(merged_data['review_datetime_utc']).dt.date
merged_data['Time'] = pd.to_datetime(merged_data['review_datetime_utc']).dt.time

# making years
merged_data['year'] = pd.DatetimeIndex(merged_data['Dates']).year

# making months
merged_data['month'] = pd.DatetimeIndex(merged_data['Dates']).month

# making days
merged_data['day'] = pd.DatetimeIndex(merged_data['Dates']).day



"""# Visualizing Statistical Variables"""

# development of avg rating per store per year
store_rating_per_year = px.histogram(merged_data, x="year", y="review_rating",
             color='place_id', barmode='group', histfunc='avg',
             height=400)
store_rating_per_year.show()

# development of avg rating per store per year
fig = px.histogram(merged_data, x="review_rating", y="review_rating",
             color='place_id', barmode='group', histfunc='count',
             height=400)
fig.show()

# owners answer during time
fig = px.histogram(merged_data, x="year", y="owner_answer",
             color='place_id', barmode='group',
             height=400)
fig.show()

options = st.multiselect(
     'Welche Stores möchten Sie miteinander vergleichen?',
     merged_data['place_id'])

st.write('Sie haben ausgewählt:', options)
